{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /opt/conda/envs/transformer/lib/python3.11/site-packages (4.33.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/transformer/lib/python3.11/site-packages (from transformers) (3.12.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/envs/transformer/lib/python3.11/site-packages (from transformers) (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/transformer/lib/python3.11/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/transformer/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/transformer/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/transformer/lib/python3.11/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/transformer/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/envs/transformer/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/transformer/lib/python3.11/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/transformer/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/transformer/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/transformer/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/transformer/lib/python3.11/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/transformer/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/transformer/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/transformer/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Answer:  {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Context: \", raw_datasets[\"train\"][0][\"context\"])\n",
    "print(\"Question: \", raw_datasets[\"train\"][0][\"question\"])\n",
    "print(\"Answer: \", raw_datasets[\"train\"][0][\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 87599\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 10570\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'What is in front of the Notre Dame Main Building?', 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?', 'What is the Grotto at Notre Dame?', 'What sits on top of the Main Building at Notre Dame?', 'When did the Scholastic Magazine of Notre dame begin publishing?', \"How often is Notre Dame's the Juggler published?\", 'What is the daily student paper at Notre Dame called?', 'How many student news papers are found at Notre Dame?', 'In what year did the student paper Common Sense begin publication at Notre Dame?']\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['train']['question'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "train_encodings = tokenizer(raw_datasets['train']['context'], raw_datasets['train']['question'], truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(raw_datasets['validation']['context'], raw_datasets['validation']['question'], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87599\n"
     ]
    }
   ],
   "source": [
    "print(len(train_encodings['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS]\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.decode(train_encodings['input_ids'][0])\n",
    "print(text[:])\n",
    "print(text[523:])\n",
    "print(tokenizer.decode(101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87599\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 0\n",
      "})\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/bert_qa_finetuning.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79657a756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3137322e32302e39332e323335227d7d/workspace/bert_qa_finetuning.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m filtered_dataset \u001b[39m=\u001b[39m raw_datasets[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mfilter(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(x[\u001b[39m\"\u001b[39m\u001b[39manswers\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79657a756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3137322e32302e39332e323335227d7d/workspace/bert_qa_finetuning.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(filtered_dataset)\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79657a756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3137322e32302e39332e323335227d7d/workspace/bert_qa_finetuning.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(raw_datasets[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39manswers\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m6\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79657a756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3137322e32302e39332e323335227d7d/workspace/bert_qa_finetuning.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(raw_datasets[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m515\u001b[39m:])\n",
      "File \u001b[0;32m/opt/conda/envs/transformer/lib/python3.11/site-packages/datasets/arrow_dataset.py:2803\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2801\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2802\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[0;32m/opt/conda/envs/transformer/lib/python3.11/site-packages/datasets/arrow_dataset.py:2788\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2786\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2787\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data, key, indices\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 2788\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2789\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39;49mformatter, format_columns\u001b[39m=\u001b[39;49mformat_columns, output_all_columns\u001b[39m=\u001b[39;49moutput_all_columns\n\u001b[1;32m   2790\u001b[0m )\n\u001b[1;32m   2791\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/opt/conda/envs/transformer/lib/python3.11/site-packages/datasets/formatting/formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    627\u001b[0m python_formatter \u001b[39m=\u001b[39m PythonFormatter(features\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m format_columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     \u001b[39mreturn\u001b[39;00m formatter(pa_table, query_type\u001b[39m=\u001b[39;49mquery_type)\n\u001b[1;32m    630\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m/opt/conda/envs/transformer/lib/python3.11/site-packages/datasets/formatting/formatting.py:398\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m    397\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_column(pa_table)\n\u001b[1;32m    399\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    400\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_batch(pa_table)\n",
      "File \u001b[0;32m/opt/conda/envs/transformer/lib/python3.11/site-packages/datasets/formatting/formatting.py:441\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_column\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m--> 441\u001b[0m     column \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpython_arrow_extractor()\u001b[39m.\u001b[39;49mextract_column(pa_table)\n\u001b[1;32m    442\u001b[0m     column \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_features_decoder\u001b[39m.\u001b[39mdecode_column(column, pa_table\u001b[39m.\u001b[39mcolumn_names[\u001b[39m0\u001b[39m])\n\u001b[1;32m    443\u001b[0m     \u001b[39mreturn\u001b[39;00m column\n",
      "File \u001b[0;32m/opt/conda/envs/transformer/lib/python3.11/site-packages/datasets/formatting/formatting.py:147\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_column\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mreturn\u001b[39;00m pa_table\u001b[39m.\u001b[39mcolumn(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto_pylist()\n",
      "File \u001b[0;32m/opt/conda/envs/transformer/lib/python3.11/site-packages/pyarrow/table.pxi:1313\u001b[0m, in \u001b[0;36mpyarrow.lib.ChunkedArray.to_pylist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/transformer/lib/python3.11/site-packages/pyarrow/array.pxi:1567\u001b[0m, in \u001b[0;36mpyarrow.lib.Array.to_pylist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/transformer/lib/python3.11/site-packages/pyarrow/scalar.pxi:750\u001b[0m, in \u001b[0;36mpyarrow.lib.StructScalar.as_py\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen _collections_abc>:786\u001b[0m, in \u001b[0;36mkeys\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(len(raw_datasets['train']))\n",
    "filtered_dataset = raw_datasets[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)\n",
    "print(filtered_dataset)\n",
    "print(raw_datasets['train']['answers'][6])\n",
    "print(raw_datasets['train']['context'][0][515:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 10567\n",
      "})\n",
      "{'text': ['location of Warsaw', 'location of Warsaw', 'location'], 'answer_start': [104, 104, 104]}\n",
      "Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n"
     ]
    }
   ],
   "source": [
    "filtered_dataset_valid = raw_datasets[\"validation\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)\n",
    "print(filtered_dataset_valid)\n",
    "print(raw_datasets['validation']['answers'][1004])\n",
    "print(raw_datasets['validation']['context'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][3276]\n",
    "raw_datasets['train'][3276]['context'][2982]\n",
    "\n",
    "# raw_datasets['train'][1]\n",
    "# raw_datasets['train'][1]['context'][212]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Skyfall'], 'answer_start': [2982], 'answer_end': [2989]}\n",
      "{'text': ['a copper statue of Christ'], 'answer_start': [188], 'answer_end': [213]}\n"
     ]
    }
   ],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "  for answer, context in zip(answers, contexts):\n",
    "    gold_text = answer['text'][0]\n",
    "    start_idx = answer['answer_start'][0]\n",
    "    end_idx = start_idx + len(gold_text)\n",
    "\n",
    "    # sometimes squad answers are off by a character or two so we fix this\n",
    "    if context[start_idx:end_idx] == gold_text:\n",
    "      answer['answer_end'] = [end_idx]\n",
    "    elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "      answer['answer_start'] = [start_idx - 1]\n",
    "      answer['answer_end'] = [end_idx - 1]     # When the gold label is off by one character\n",
    "    elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "      answer['answer_start'] = [start_idx - 2]\n",
    "      answer['answer_end'] = [end_idx - 2]     # When the gold label is off by two characters\n",
    "\n",
    "train_context = raw_datasets['train']['context']\n",
    "train_answers = raw_datasets['train']['answers']\n",
    "\n",
    "add_end_idx(train_answers, train_context)\n",
    "print(train_answers[3276])\n",
    "print(train_answers[1])\n",
    "# add_end_idx(raw_datasets['validation']['answers'], raw_datasets['validation']['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "a\n",
      "45\n",
      "Christ\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings.char_to_token(3276, 2982)) # (batch num , char start index)\n",
    "print(tokenizer.decode(train_encodings['input_ids'][1][41]))\n",
    "print(train_encodings.char_to_token(1, 212))\n",
    "print(tokenizer.decode(train_encodings['input_ids'][1][45]))\n",
    "print(tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i, answer in enumerate(answers):\n",
    "        if encodings.char_to_token(i, answer['answer_start'][0]) == None or encodings.char_to_token(i, answer['answer_end'][0] - 1) == None:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            start_positions.append(encodings.char_to_token(i, answer['answer_start'][0]))\n",
    "            end_positions.append(encodings.char_to_token(i, answer['answer_end'][0] - 1))\n",
    "        \n",
    "    if start_positions[-1] is None:\n",
    "        start_positions[-1] = tokenizer.model_max_length\n",
    "    if end_positions[-1] is None:\n",
    "        end_positions[-1] = tokenizer.model_max_length    \n",
    "    \n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "        \n",
    "add_token_positions(train_encodings, train_answers)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[126,\n",
       " 45,\n",
       " 69,\n",
       " 96,\n",
       " 27,\n",
       " 53,\n",
       " 89,\n",
       " 117,\n",
       " 27,\n",
       " 173,\n",
       " 23,\n",
       " 32,\n",
       " 50,\n",
       " 76,\n",
       " 150,\n",
       " 93,\n",
       " 8,\n",
       " 26,\n",
       " 59,\n",
       " 29,\n",
       " 97,\n",
       " 16,\n",
       " 37,\n",
       " 126,\n",
       " 75,\n",
       " 122,\n",
       " 222,\n",
       " 22,\n",
       " 155,\n",
       " 12,\n",
       " 110,\n",
       " 59,\n",
       " 81,\n",
       " 76,\n",
       " 26,\n",
       " 35,\n",
       " 39,\n",
       " 73,\n",
       " 84,\n",
       " 24,\n",
       " 36,\n",
       " 57,\n",
       " 109,\n",
       " 138,\n",
       " 10,\n",
       " 80,\n",
       " 121,\n",
       " 39,\n",
       " 186,\n",
       " 10,\n",
       " 72,\n",
       " 85,\n",
       " 127,\n",
       " 25,\n",
       " 2,\n",
       " 40,\n",
       " 36,\n",
       " 59,\n",
       " 21,\n",
       " 5,\n",
       " 195,\n",
       " 209,\n",
       " 220,\n",
       " 22,\n",
       " 4,\n",
       " 18,\n",
       " 44,\n",
       " 4,\n",
       " 18,\n",
       " 25,\n",
       " 83,\n",
       " 138,\n",
       " 165,\n",
       " 19,\n",
       " 57,\n",
       " 302,\n",
       " 334,\n",
       " 12,\n",
       " 3,\n",
       " 29,\n",
       " 70,\n",
       " 9,\n",
       " 75,\n",
       " 25,\n",
       " 52,\n",
       " 78,\n",
       " 268,\n",
       " 62,\n",
       " 134,\n",
       " 18,\n",
       " 51,\n",
       " 76,\n",
       " 171,\n",
       " 48,\n",
       " 121,\n",
       " 5,\n",
       " 15,\n",
       " 87,\n",
       " 28,\n",
       " 46,\n",
       " 77,\n",
       " 58,\n",
       " 63,\n",
       " 58,\n",
       " 111,\n",
       " 251,\n",
       " 169,\n",
       " 172,\n",
       " 8,\n",
       " 8,\n",
       " 69,\n",
       " 73,\n",
       " 129,\n",
       " 93,\n",
       " 115,\n",
       " 156,\n",
       " 16,\n",
       " 122,\n",
       " 19,\n",
       " 48,\n",
       " 73,\n",
       " 30,\n",
       " 120,\n",
       " 21,\n",
       " 135,\n",
       " 209,\n",
       " 260,\n",
       " 36,\n",
       " 23,\n",
       " 72,\n",
       " 92,\n",
       " 130,\n",
       " 50,\n",
       " 2,\n",
       " 23,\n",
       " 36,\n",
       " 100,\n",
       " 120,\n",
       " 35,\n",
       " 43,\n",
       " 73,\n",
       " 95,\n",
       " 153,\n",
       " 16,\n",
       " 20,\n",
       " 42,\n",
       " 81,\n",
       " 133,\n",
       " 22,\n",
       " 64,\n",
       " 74,\n",
       " 120,\n",
       " 345,\n",
       " 28,\n",
       " 95,\n",
       " 127,\n",
       " 162,\n",
       " 35,\n",
       " 62,\n",
       " 106,\n",
       " 38,\n",
       " 6,\n",
       " 17,\n",
       " 24,\n",
       " 45,\n",
       " 91,\n",
       " 16,\n",
       " 11,\n",
       " 25,\n",
       " 48,\n",
       " 54,\n",
       " 4,\n",
       " 30,\n",
       " 36,\n",
       " 58,\n",
       " 117,\n",
       " 78,\n",
       " 87,\n",
       " 113,\n",
       " 134,\n",
       " 8,\n",
       " 32,\n",
       " 48,\n",
       " 101,\n",
       " 116,\n",
       " 6,\n",
       " 32,\n",
       " 59,\n",
       " 73,\n",
       " 81,\n",
       " 16,\n",
       " 30,\n",
       " 44,\n",
       " 68,\n",
       " 85,\n",
       " 30,\n",
       " 52,\n",
       " 90,\n",
       " 101,\n",
       " 168,\n",
       " 2,\n",
       " 21,\n",
       " 36,\n",
       " 71,\n",
       " 86,\n",
       " 10,\n",
       " 66,\n",
       " 95,\n",
       " 98,\n",
       " 85,\n",
       " 8,\n",
       " 13,\n",
       " 34,\n",
       " 103,\n",
       " 12,\n",
       " 35,\n",
       " 80,\n",
       " 52,\n",
       " 72,\n",
       " 21,\n",
       " 57,\n",
       " 193,\n",
       " 206,\n",
       " 84,\n",
       " 48,\n",
       " 72,\n",
       " 137,\n",
       " 241,\n",
       " 275,\n",
       " 15,\n",
       " 50,\n",
       " 83,\n",
       " 99,\n",
       " 140,\n",
       " 19,\n",
       " 26,\n",
       " 96,\n",
       " 136,\n",
       " 81,\n",
       " 4,\n",
       " 89,\n",
       " 118,\n",
       " 131,\n",
       " 146,\n",
       " 41,\n",
       " 57,\n",
       " 129,\n",
       " 112,\n",
       " 90,\n",
       " 11,\n",
       " 17,\n",
       " 29,\n",
       " 36,\n",
       " 187,\n",
       " 54,\n",
       " 40,\n",
       " 37,\n",
       " 90,\n",
       " 157,\n",
       " 73,\n",
       " 92,\n",
       " 131,\n",
       " 106,\n",
       " 17,\n",
       " 36,\n",
       " 57,\n",
       " 79,\n",
       " 194,\n",
       " 260,\n",
       " 64,\n",
       " 51,\n",
       " 125,\n",
       " 43,\n",
       " 64,\n",
       " 78,\n",
       " 123,\n",
       " 88,\n",
       " 41,\n",
       " 123,\n",
       " 23,\n",
       " 6,\n",
       " 64,\n",
       " 67,\n",
       " 123,\n",
       " 125,\n",
       " 138,\n",
       " 67,\n",
       " 123,\n",
       " 125,\n",
       " 59,\n",
       " 99,\n",
       " 149,\n",
       " 71,\n",
       " 153,\n",
       " 55,\n",
       " 108,\n",
       " 13,\n",
       " 24,\n",
       " 71,\n",
       " 99,\n",
       " 125,\n",
       " 29,\n",
       " 190,\n",
       " 200,\n",
       " 148,\n",
       " 200,\n",
       " 9,\n",
       " 198,\n",
       " 80,\n",
       " 91,\n",
       " 80,\n",
       " 115,\n",
       " 200,\n",
       " 83,\n",
       " 61,\n",
       " 90,\n",
       " 145,\n",
       " 44,\n",
       " 34,\n",
       " 70,\n",
       " 138,\n",
       " 44,\n",
       " 33,\n",
       " 70,\n",
       " 139,\n",
       " 145,\n",
       " 12,\n",
       " 33,\n",
       " 106,\n",
       " 33,\n",
       " 156,\n",
       " 104,\n",
       " 77,\n",
       " 12,\n",
       " 33,\n",
       " 69,\n",
       " 156,\n",
       " 71,\n",
       " 123,\n",
       " 199,\n",
       " 71,\n",
       " 119,\n",
       " 271,\n",
       " 199,\n",
       " 3,\n",
       " 3,\n",
       " 44,\n",
       " 71,\n",
       " 119,\n",
       " 251,\n",
       " 51,\n",
       " 213,\n",
       " 293,\n",
       " 12,\n",
       " 22,\n",
       " 51,\n",
       " 212,\n",
       " 293,\n",
       " 23,\n",
       " 52,\n",
       " 77,\n",
       " 165,\n",
       " 293,\n",
       " 34,\n",
       " 66,\n",
       " 139,\n",
       " 43,\n",
       " 82,\n",
       " 139,\n",
       " 31,\n",
       " 32,\n",
       " 139,\n",
       " 27,\n",
       " 10,\n",
       " 47,\n",
       " 73,\n",
       " 175,\n",
       " 120,\n",
       " 126,\n",
       " 27,\n",
       " 86,\n",
       " 126,\n",
       " 137,\n",
       " 237,\n",
       " 16,\n",
       " 72,\n",
       " 95,\n",
       " 154,\n",
       " 183,\n",
       " 27,\n",
       " 43,\n",
       " 90,\n",
       " 95,\n",
       " 117,\n",
       " 27,\n",
       " 13,\n",
       " 50,\n",
       " 95,\n",
       " 95,\n",
       " 32,\n",
       " 51,\n",
       " 96,\n",
       " 110,\n",
       " 32,\n",
       " 12,\n",
       " 51,\n",
       " 32,\n",
       " 260,\n",
       " 12,\n",
       " 58,\n",
       " 110,\n",
       " 261,\n",
       " 169,\n",
       " 115,\n",
       " 272,\n",
       " 3,\n",
       " 151,\n",
       " 214,\n",
       " 272,\n",
       " 13,\n",
       " 25,\n",
       " 57,\n",
       " 195,\n",
       " 36,\n",
       " 76,\n",
       " 86,\n",
       " 86,\n",
       " 29,\n",
       " 81,\n",
       " 86,\n",
       " 9,\n",
       " 36,\n",
       " 81,\n",
       " 154,\n",
       " 14,\n",
       " 37,\n",
       " 37,\n",
       " 111,\n",
       " 134,\n",
       " 25,\n",
       " 118,\n",
       " 193,\n",
       " 14,\n",
       " 89,\n",
       " 98,\n",
       " 118,\n",
       " 193,\n",
       " 10,\n",
       " 69,\n",
       " 220,\n",
       " 373,\n",
       " 447,\n",
       " 21,\n",
       " 415,\n",
       " 373,\n",
       " 447,\n",
       " 5,\n",
       " 11,\n",
       " 40,\n",
       " 145,\n",
       " 389,\n",
       " 14,\n",
       " 79,\n",
       " 107,\n",
       " 135,\n",
       " 229,\n",
       " 79,\n",
       " 139,\n",
       " 144,\n",
       " 192,\n",
       " 14,\n",
       " 79,\n",
       " 120,\n",
       " 140,\n",
       " 11,\n",
       " 66,\n",
       " 88,\n",
       " 111,\n",
       " 126,\n",
       " 12,\n",
       " 93,\n",
       " 88,\n",
       " 126,\n",
       " 66,\n",
       " 11,\n",
       " 66,\n",
       " 88,\n",
       " 126,\n",
       " 11,\n",
       " 11,\n",
       " 67,\n",
       " 11,\n",
       " 16,\n",
       " 36,\n",
       " 54,\n",
       " 4,\n",
       " 16,\n",
       " 40,\n",
       " 54,\n",
       " 2,\n",
       " 78,\n",
       " 103,\n",
       " 67,\n",
       " 121,\n",
       " 11,\n",
       " 2,\n",
       " 67,\n",
       " 103,\n",
       " 36,\n",
       " 11,\n",
       " 79,\n",
       " 95,\n",
       " 12,\n",
       " 86,\n",
       " 151,\n",
       " 178,\n",
       " 12,\n",
       " 22,\n",
       " 160,\n",
       " 12,\n",
       " 5,\n",
       " 12,\n",
       " 21,\n",
       " 140,\n",
       " 178,\n",
       " 5,\n",
       " 26,\n",
       " 20,\n",
       " 49,\n",
       " 5,\n",
       " 20,\n",
       " 32,\n",
       " 39,\n",
       " 5,\n",
       " 20,\n",
       " 30,\n",
       " 49,\n",
       " 38,\n",
       " 18,\n",
       " 154,\n",
       " 252,\n",
       " 3,\n",
       " 33,\n",
       " 52,\n",
       " 117,\n",
       " 3,\n",
       " 33,\n",
       " 41,\n",
       " 79,\n",
       " 154,\n",
       " 22,\n",
       " 8,\n",
       " 128,\n",
       " 106,\n",
       " 13,\n",
       " 92,\n",
       " 97,\n",
       " 128,\n",
       " 10,\n",
       " 22,\n",
       " 68,\n",
       " 93,\n",
       " 97,\n",
       " 18,\n",
       " 5,\n",
       " 18,\n",
       " 169,\n",
       " 282,\n",
       " 314,\n",
       " 5,\n",
       " 82,\n",
       " 164,\n",
       " 205,\n",
       " 21,\n",
       " 66,\n",
       " 83,\n",
       " 160,\n",
       " 21,\n",
       " 66,\n",
       " 160,\n",
       " 17,\n",
       " 83,\n",
       " 160,\n",
       " 160,\n",
       " 16,\n",
       " 21,\n",
       " 66,\n",
       " 83,\n",
       " 160,\n",
       " 28,\n",
       " 31,\n",
       " 5,\n",
       " 22,\n",
       " 28,\n",
       " 25,\n",
       " 5,\n",
       " 28,\n",
       " 97,\n",
       " 175,\n",
       " 128,\n",
       " 12,\n",
       " 74,\n",
       " 97,\n",
       " 156,\n",
       " 175,\n",
       " 22,\n",
       " 74,\n",
       " 97,\n",
       " 128,\n",
       " 193,\n",
       " 12,\n",
       " 82,\n",
       " 119,\n",
       " 12,\n",
       " 94,\n",
       " 137,\n",
       " 162,\n",
       " 21,\n",
       " 12,\n",
       " 21,\n",
       " 119,\n",
       " 162,\n",
       " 56,\n",
       " 38,\n",
       " 16,\n",
       " 22,\n",
       " 38,\n",
       " 16,\n",
       " 109,\n",
       " 5,\n",
       " 16,\n",
       " 38,\n",
       " 87,\n",
       " 109,\n",
       " 189,\n",
       " 23,\n",
       " 85,\n",
       " 144,\n",
       " 175,\n",
       " 23,\n",
       " 23,\n",
       " 36,\n",
       " 98,\n",
       " 144,\n",
       " 6,\n",
       " 128,\n",
       " 6,\n",
       " 9,\n",
       " 127,\n",
       " 111,\n",
       " 140,\n",
       " 91,\n",
       " 127,\n",
       " 11,\n",
       " 82,\n",
       " 44,\n",
       " 71,\n",
       " 2,\n",
       " 37,\n",
       " 56,\n",
       " 82,\n",
       " 11,\n",
       " 37,\n",
       " 56,\n",
       " 44,\n",
       " 6,\n",
       " 28,\n",
       " 6,\n",
       " 43,\n",
       " 57,\n",
       " 228,\n",
       " 324,\n",
       " 415,\n",
       " 57,\n",
       " 1,\n",
       " 244,\n",
       " 368,\n",
       " 415,\n",
       " 1,\n",
       " 346,\n",
       " 317,\n",
       " 266,\n",
       " 7,\n",
       " 14,\n",
       " 95,\n",
       " 123,\n",
       " 10,\n",
       " 102,\n",
       " 164,\n",
       " 201,\n",
       " 7,\n",
       " 78,\n",
       " 144,\n",
       " 165,\n",
       " 9,\n",
       " 19,\n",
       " 65,\n",
       " 100,\n",
       " 116,\n",
       " 9,\n",
       " 59,\n",
       " 65,\n",
       " 83,\n",
       " 9,\n",
       " 65,\n",
       " 117,\n",
       " 101,\n",
       " 109,\n",
       " 73,\n",
       " 34,\n",
       " 66,\n",
       " 83,\n",
       " 117,\n",
       " 40,\n",
       " 82,\n",
       " 119,\n",
       " 43,\n",
       " 41,\n",
       " 101,\n",
       " 157,\n",
       " 2,\n",
       " 46,\n",
       " 139,\n",
       " 47,\n",
       " 25,\n",
       " 42,\n",
       " 53,\n",
       " 144,\n",
       " 4,\n",
       " 13,\n",
       " 4,\n",
       " 131,\n",
       " 4,\n",
       " 119,\n",
       " 4,\n",
       " 4,\n",
       " 74,\n",
       " 86,\n",
       " 119,\n",
       " 6,\n",
       " 31,\n",
       " 97,\n",
       " 26,\n",
       " 102,\n",
       " 53,\n",
       " 32,\n",
       " 46,\n",
       " 14,\n",
       " 135,\n",
       " 33,\n",
       " 14,\n",
       " 43,\n",
       " 130,\n",
       " 149,\n",
       " 56,\n",
       " 14,\n",
       " 33,\n",
       " 93,\n",
       " 140,\n",
       " 14,\n",
       " 53,\n",
       " 72,\n",
       " 2,\n",
       " 14,\n",
       " 14,\n",
       " 84,\n",
       " 14,\n",
       " 14,\n",
       " 25,\n",
       " 53,\n",
       " 84,\n",
       " 10,\n",
       " 103,\n",
       " 8,\n",
       " 21,\n",
       " 112,\n",
       " 88,\n",
       " 21,\n",
       " 103,\n",
       " 32,\n",
       " 95,\n",
       " 120,\n",
       " 167,\n",
       " 56,\n",
       " 159,\n",
       " 117,\n",
       " 106,\n",
       " 94,\n",
       " 130,\n",
       " 11,\n",
       " 19,\n",
       " 92,\n",
       " 118,\n",
       " 92,\n",
       " 92,\n",
       " 128,\n",
       " 19,\n",
       " 92,\n",
       " 128,\n",
       " 53,\n",
       " 9,\n",
       " 21,\n",
       " 35,\n",
       " 54,\n",
       " 81,\n",
       " 35,\n",
       " 48,\n",
       " 54,\n",
       " 86,\n",
       " 3,\n",
       " 35,\n",
       " 45,\n",
       " 109,\n",
       " 25,\n",
       " 107,\n",
       " 40,\n",
       " 80,\n",
       " 107,\n",
       " 107,\n",
       " 110,\n",
       " 4,\n",
       " 26,\n",
       " 79,\n",
       " 4,\n",
       " 26,\n",
       " 4,\n",
       " 26,\n",
       " 53,\n",
       " 37,\n",
       " 55,\n",
       " 62,\n",
       " 2,\n",
       " 28,\n",
       " 55,\n",
       " 62,\n",
       " 55,\n",
       " 38,\n",
       " 50,\n",
       " 20,\n",
       " 25,\n",
       " 72,\n",
       " 138,\n",
       " 99,\n",
       " 10,\n",
       " 25,\n",
       " 72,\n",
       " 86,\n",
       " 111,\n",
       " 51,\n",
       " 171,\n",
       " 266,\n",
       " 58,\n",
       " 171,\n",
       " 266,\n",
       " 184,\n",
       " 9,\n",
       " 51,\n",
       " 178,\n",
       " 184,\n",
       " 266,\n",
       " 79,\n",
       " 101,\n",
       " 135,\n",
       " 136,\n",
       " 79,\n",
       " 4,\n",
       " 120,\n",
       " 79,\n",
       " 90,\n",
       " 101,\n",
       " 121,\n",
       " 125,\n",
       " 8,\n",
       " 61,\n",
       " 79,\n",
       " 219,\n",
       " 243,\n",
       " 8,\n",
       " 63,\n",
       " 80,\n",
       " 164,\n",
       " 47,\n",
       " 164,\n",
       " 219,\n",
       " 16,\n",
       " 25,\n",
       " 79,\n",
       " 32,\n",
       " 1,\n",
       " 16,\n",
       " 25,\n",
       " 32,\n",
       " 241,\n",
       " 81,\n",
       " 16,\n",
       " 25,\n",
       " 60,\n",
       " 79,\n",
       " 241,\n",
       " 4,\n",
       " 38,\n",
       " 49,\n",
       " 141,\n",
       " 220,\n",
       " 6,\n",
       " 50,\n",
       " 69,\n",
       " 169,\n",
       " 4,\n",
       " 49,\n",
       " 62,\n",
       " 182,\n",
       " 7,\n",
       " 44,\n",
       " 66,\n",
       " 125,\n",
       " 9,\n",
       " 34,\n",
       " 51,\n",
       " 60,\n",
       " 7,\n",
       " 44,\n",
       " 66,\n",
       " 106,\n",
       " 8,\n",
       " 1,\n",
       " 62,\n",
       " 137,\n",
       " 177,\n",
       " 62,\n",
       " 106,\n",
       " 128,\n",
       " 154,\n",
       " 46,\n",
       " 65,\n",
       " 62,\n",
       " 153,\n",
       " 96,\n",
       " 11,\n",
       " 96,\n",
       " 37,\n",
       " 51,\n",
       " 11,\n",
       " 11,\n",
       " 26,\n",
       " 36,\n",
       " 57,\n",
       " 29,\n",
       " 47,\n",
       " 63,\n",
       " 145,\n",
       " 29,\n",
       " 145,\n",
       " 16,\n",
       " 29,\n",
       " 22,\n",
       " 63,\n",
       " 5,\n",
       " 51,\n",
       " 5,\n",
       " 28,\n",
       " 172,\n",
       " 31,\n",
       " 62,\n",
       " 51,\n",
       " 159,\n",
       " 4,\n",
       " 52,\n",
       " 93,\n",
       " 169,\n",
       " 183,\n",
       " 4,\n",
       " 21,\n",
       " 52,\n",
       " 135,\n",
       " 4,\n",
       " 117,\n",
       " 14,\n",
       " 179,\n",
       " 69,\n",
       " 65,\n",
       " 90,\n",
       " 157,\n",
       " 11,\n",
       " 42,\n",
       " 90,\n",
       " 167,\n",
       " 140,\n",
       " 23,\n",
       " 54,\n",
       " 38,\n",
       " 28,\n",
       " 62,\n",
       " 23,\n",
       " 38,\n",
       " 13,\n",
       " 3,\n",
       " 34,\n",
       " 57,\n",
       " 13,\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings['end_positions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings['start_positions'][3276])\n",
    "print(train_encodings['end_positions'][3276])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_answer(idx):\n",
    "    print(tokenizer.decode(train_encodings['input_ids'][idx][train_encodings['start_positions'][idx]: train_encodings['end_positions'][idx] + 1]))\n",
    "    print(train_answers[idx]['text'][0])\n",
    "    print(tokenizer.decode(train_encodings['input_ids'][idx][train_encodings['start_positions'][idx]: train_encodings['end_positions'][idx] + 1]) == train_answers[idx]['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "75\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "show_answer(3275)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Christ'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_encodings['input_ids'][1][train_encodings['start_positions'][1]: train_encodings['end_positions'][1] + 1])\n",
    "tokenizer.decode(train_encodings['input_ids'][1][train_encodings['end_positions'][1]])\n",
    "# tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SQuAD_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SQuAD_Dataset(train_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101, 15247, 12647, 14089, 11794,  1104,  1103,  1273,  1108,  3216,\n",
       "          1107,  1103,  1244,  1311,   119,  1130,   170,   181, 16140, 18900,\n",
       "          3189,  1111,  4271,  2036,  7488,   119,  3254,   117,  3895,   163,\n",
       "         12666,  1200, 22087,  8976,  1522,  1103,  1273,   123,   119,   126,\n",
       "          2940,  1149,  1104,   125,   117,  7645,   156, 26426,  1874,  1112,\n",
       "         22410,  1105,  3372,  1106,  2364,  4862,  1113,  1157,  3209,   119,\n",
       "          8488, 17037,  4047,   117, 19730,  1103,  1273,  1111,  2238,  2460,\n",
       "          2706,   117,  4803,  1115,   156, 26426,  1874,   107,  2502,  1228,\n",
       "          1112,  8984,  1105,  8362,  4935, 23709,   107,   119,  2268, 10559,\n",
       "          1742, 23612, 25019,  1104,  1109,  1203,  1365,  2706, 13316,  3540,\n",
       "          1103,  1273,  1112,  1515,   107,  1720, 11567,   107,  1105, 21718,\n",
       "          1665,  2047, 21361,  1158,  1157,  1560,  1785,  1111,  1103,  8590,\n",
       "          1104,  2884,  1701,  5166,   119, 10754,   112,  2796,  3401,  6738,\n",
       "          2142,  1145,  3777, 10543,  1103,  1273,   117, 10552,  6094, 16368,\n",
       "           156, 26426,  1874,  1112,   107,  1103,  4997,  3135,  1559,  2523,\n",
       "          1107,  1476,  1201,   107,   119, 12413, 13359,  7192,  1732,  1104,\n",
       "          4549,  8922,  6497,   156, 26426,  1874,  1112,   107,  1126,  1166,\n",
       "         11811,  5796,  1106,  1412,  1954,  3510, 27338,  1721,   107,   117,\n",
       "         25850,   107,  1106,  1129,   170,  7838,  2200,  8047,   107,  1105,\n",
       "         15975,   107,  2111,  1112,   170, 18913,   107,   119,  1799,  9095,\n",
       "          1115,   107,   164,   183,   166,   184, 25202,  1115,  5940,  1107,\n",
       "           156, 26426,  1874,  3486,  1146,  1106,  1256,  3137, 11730, 19258,\n",
       "           107,   117,  1119,  1125,   107,  1435,  1136,  1106, 16712,   156,\n",
       "         26426,  1874,   117,  1133,  1106,  6994,  1193,  9010,  1122,   119,\n",
       "          2279,  1103,  1509,  2496,  1104,  1103,  2523,  1110,  1177,  4020,\n",
       "           117,  1177,  1209,  5834,   184, 21238,  5613,   117,  1115,  1122,\n",
       "         18641,  3908,  2209,   119,   107,  1130,   170,  3112,  3189,  8782,\n",
       "          4118,   117,  1943,   157, 22116,  1733,  1522,  1103,  1273,   124,\n",
       "           119,   126,  2940,  1149,  1104,   125,   117,  7645,   107,  1109,\n",
       "         13722,  2523,  1164,  1103,  1418, 26574,  1545,  3677,  1114,   170,\n",
       "          5941,  1106,  2311,  1110,  1710,  1159,  1111,  8211,  3899,   117,\n",
       "           170,  9250,   117,  6276,   117, 10144,  1193,  1666,   191,  7531,\n",
       "         26728,  1162,  1106,  1103,  6119,   118,  1919,  5801,  1107,  5558,\n",
       "           107,   119,  2189,  3112,  3761,  1121, 10829,  2001,  1708, 22096,\n",
       "          1121,  1103,  1727,  2948, 11047,   117,  1522,  1122,   170,  3264,\n",
       "          1620,  2794,   117,  4797,   131,   789,  1448,  1104,  1103,  1632,\n",
       "         10241,  1116,  1104,   156, 26426,  1874,  1110,  1115,   117,  1107,\n",
       "          1901,  1106,  1155,  1103, 20329,  2168,   117,  1105,  1155,  1103,\n",
       "          1159,  1193,  7732,  1106,   170,  3318,  2369,  1149,  1106,  8991,\n",
       "          2490,   787,   188,  2357,  1869,   117,  1195,  1243,  1106,  2059,\n",
       "          1107,  8211,  1112,   170,  1825,   119,   790,  3620,   160, 17481,\n",
       "          2340,  1121,  1103,  1203,  1365,  5732,  3128,   117,  1522,  1122,\n",
       "          1126,  2908,  3654,   117,  2157,   131,   789,  6422,  1110, 11224,\n",
       "          1193,  7856,   119,  4111, 18757, 16065,  8419,  2228,   170,  1363,\n",
       "           117, 22161,  5077,  1830,   118,  1176, 15467,   119,  1262,  1229,\n",
       "         12958, 22087, 19429, 24060,  2144,   787,   189,  1817,   170,  3321,\n",
       "          8351,  1112,  1142,  1273,   787,   188,   789,  8211,  1873,   117,\n",
       "           790,  3229,  1122,   787,   188,  1272,  1195,   787,  1396,  1640,\n",
       "          1899,   783,  1677,  1315,  4016,   783,  1103,   177,  1183,  1643,\n",
       "         12512,  1596,  8958,  4720, 15072,   102,  1327,  2794,  1225,  1103,\n",
       "          2432,  1121,  1103,  2290, 11693,  1660,  1106,   156, 26426,  1874,\n",
       "           136,   102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'start_positions': tensor(0),\n",
       " 'end_positions': tensor(0)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[3275]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Working on {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/transformer/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5475/5475 [33:20<00:00,  2.74it/s, loss=1.22] \n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5475/5475 [33:22<00:00,  2.73it/s, loss=1.03] \n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5475/5475 [33:17<00:00,  2.74it/s, loss=1.36] \n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5475/5475 [33:23<00:00,  2.73it/s, loss=0.389] \n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5475/5475 [33:21<00:00,  2.74it/s, loss=0.272] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm # ì§„í–‰ë¥ \n",
    "\n",
    "N_EPOCHS = 5\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "  loop = tqdm(train_loader, leave=True)\n",
    "  for batch in loop:\n",
    "    optim.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    start_positions = batch['start_positions'].to(device)\n",
    "    end_positions = batch['end_positions'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    loop.set_description(f'Epoch {epoch+1}')\n",
    "    loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model_path = os.getcwd()\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/workspace/BERT-SQUAD/tokenizer_config.json',\n",
       " '/workspace/BERT-SQUAD/special_tokens_map.json',\n",
       " '/workspace/BERT-SQUAD/vocab.txt',\n",
       " '/workspace/BERT-SQUAD/added_tokens.json',\n",
       " '/workspace/BERT-SQUAD/tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '/workspace/BERT-SQUAD/'\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForQuestionAnswering(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizerFast\n",
    "\n",
    "model_path = '/workspace/BERT-SQUAD'\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(context, question):\n",
    "  inputs = tokenizer.encode_plus(question, context, return_tensors='pt').to(device)\n",
    "  outputs = model(**inputs)\n",
    "  \n",
    "  answer_start = torch.argmax(outputs[0])  \n",
    "  answer_end = torch.argmax(outputs[1]) + 1 \n",
    "  \n",
    "  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "  \n",
    "  return answer\n",
    "\n",
    "def normalize_text(s):\n",
    "  \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "  import string, re\n",
    "  def remove_articles(text):\n",
    "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "    return re.sub(regex, \" \", text)\n",
    "  def white_space_fix(text):\n",
    "    return \" \".join(text.split())\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return \"\".join(ch for ch in text if ch not in exclude)\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match(prediction, truth):\n",
    "    return bool(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "  pred_tokens = normalize_text(prediction).split()\n",
    "  truth_tokens = normalize_text(truth).split()\n",
    "  \n",
    "  # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "  if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "    return int(pred_tokens == truth_tokens)\n",
    "  \n",
    "  common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "  \n",
    "  # if there are no common tokens then f1 = 0\n",
    "  if len(common_tokens) == 0:\n",
    "    return 0\n",
    "  \n",
    "  prec = len(common_tokens) / len(pred_tokens)\n",
    "  rec = len(common_tokens) / len(truth_tokens)\n",
    "  \n",
    "  return round(2 * (prec * rec) / (prec + rec), 2)\n",
    "  \n",
    "def question_answer(context, question,answer):\n",
    "  prediction = get_prediction(context,question)\n",
    "  em_score = exact_match(prediction, answer)\n",
    "  f1_score = compute_f1(prediction, answer)\n",
    "\n",
    "  print(f'Question: {question}')\n",
    "  print(f'Prediction: {prediction}')\n",
    "  print(f'True Answer: {answer}')\n",
    "  print(f'Exact match: {em_score}')\n",
    "  print(f'F1 score: {f1_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For whom the passage is talking about?\n",
      "Prediction: BeyoncÃ© Giselle Knowles - Carter\n",
      "True Answer: Beyonce Giselle Knowles - Carter\n",
      "Exact match: False\n",
      "F1 score: 0.75\n",
      "\n",
      "Question: When did Beyonce born?\n",
      "Prediction: September 4, 1981\n",
      "True Answer: September 4, 1981\n",
      "Exact match: True\n",
      "F1 score: 1.0\n",
      "\n",
      "Question: Where did Beyonce born?\n",
      "Prediction: Houston, Texas\n",
      "True Answer: Houston, Texas\n",
      "Exact match: True\n",
      "F1 score: 1.0\n",
      "\n",
      "Question: What is Beyonce's nationality?\n",
      "Prediction: American\n",
      "True Answer: American\n",
      "Exact match: True\n",
      "F1 score: 1.0\n",
      "\n",
      "Question: Who was the Destiny's group manager?\n",
      "Prediction: \n",
      "True Answer: Mathew Knowles\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What name has the BeyoncÃ©'s debut album?\n",
      "Prediction: Dangerously in Love\n",
      "True Answer: Dangerously in Love\n",
      "Exact match: True\n",
      "F1 score: 1.0\n",
      "\n",
      "Question: How many Grammy Awards did Beyonce earn?\n",
      "Prediction: five\n",
      "True Answer: five\n",
      "Exact match: True\n",
      "F1 score: 1.0\n",
      "\n",
      "Question: When did the BeyoncÃ©'s debut album release?\n",
      "Prediction: 2003\n",
      "True Answer: 2003\n",
      "Exact match: True\n",
      "F1 score: 1.0\n",
      "\n",
      "Question: Who was the lead singer of R&B girl-group Destiny's Child?\n",
      "Prediction: \n",
      "True Answer: Beyonce Giselle Knowles - Carter\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"BeyoncÃ© Giselle Knowles-Carter (/biËËˆjÉ’nseÉª/ bee-YON-say) (born September 4, 1981) is an American singer, \n",
    "          songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing \n",
    "          and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. \n",
    "          Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. \n",
    "          Their hiatus saw the release of BeyoncÃ©\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, \n",
    "          earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\"\"\"\n",
    "\n",
    "\n",
    "questions = [\"For whom the passage is talking about?\",\n",
    "             \"When did Beyonce born?\",\n",
    "             \"Where did Beyonce born?\",\n",
    "             \"What is Beyonce's nationality?\",\n",
    "             \"Who was the Destiny's group manager?\",\n",
    "             \"What name has the BeyoncÃ©'s debut album?\",\n",
    "             \"How many Grammy Awards did Beyonce earn?\",\n",
    "             \"When did the BeyoncÃ©'s debut album release?\",\n",
    "             \"Who was the lead singer of R&B girl-group Destiny's Child?\"]\n",
    "\n",
    "answers = [\"Beyonce Giselle Knowles - Carter\", \"September 4, 1981\", \"Houston, Texas\", \n",
    "           \"American\", \"Mathew Knowles\", \"Dangerously in Love\", \"five\", \"2003\", \n",
    "           \"Beyonce Giselle Knowles - Carter\"]\n",
    "\n",
    "for question, answer in zip(questions, answers):\n",
    "  question_answer(context, question, answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
